{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chuyển đổi thành công từ /home/tuandinh/Desktop/Signal Classification/data_signal_classification.txt thành /home/tuandinh/Desktop/Signal Classification/data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Đường dẫn đến tệp văn bản đầu vào\n",
    "input_txt_file = '/home/tuandinh/Desktop/Signal Classification/data_signal_classification.txt'\n",
    "\n",
    "# Đường dẫn đến tệp CSV đầu ra\n",
    "output_csv_file = '/home/tuandinh/Desktop/Signal Classification/data.csv'\n",
    "\n",
    "# Mở tệp văn bản đầu vào để đọc\n",
    "with open(input_txt_file, 'r') as txtfile:\n",
    "    # Đọc từng dòng trong tệp văn bản\n",
    "    lines = txtfile.readlines()\n",
    "\n",
    "# Mở tệp CSV đầu ra để viết\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    csvwriter.writerow(['Frequency', 'No_defect', 'with_defect'])\n",
    "\n",
    "    # Viết dữ liệu từ tệp văn bản vào tệp CSV\n",
    "    for line in lines:\n",
    "        # Tách dữ liệu bằng dấu phân tách của bạn (ví dụ: dấu tab '\\t')\n",
    "        data = line.strip().split('\\t')\n",
    "\n",
    "        # Viết dữ liệu vào tệp CSV\n",
    "        csvwriter.writerow(data)\n",
    "\n",
    "print(f\"Chuyển đổi thành công từ {input_txt_file} thành {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data_frame.iloc[index, :].values.astype(float)\n",
    "\n",
    "        # The last column can be the target label (0 for no defect, 1 for defect)\n",
    "        # You can adjust this based on your actual data structure\n",
    "        target = int(sample[-1])\n",
    "\n",
    "        # Remove the target label from the sample\n",
    "        sample = sample[:-1]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        # print(f\"Length of sample {len(sample)}\")\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        # Apply the optional data transformation\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "# Path to your merged CSV file\n",
    "# csv_merged = '/home/tuandinh/Desktop/Signal Classification/data_signal.csv'\n",
    "csv_merged = '/home/tuandinh/Desktop/Signal Classification/file_data_new.csv'\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = SignalDataset(csv_merged)\n",
    "# test_dataset = SignalDataset(csv_test_path)\n",
    "\n",
    "# # Create a data loader for the dataset\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a perceptron \n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(163, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)   \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleClassifier(\n",
       "  (fc1): Linear(in_features=163, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_size = 163  # Number of features in your data\n",
    "hidden_size = 64  # Number of hidden units in the neural network\n",
    "num_classes = 2  # Number of classes (0 for no defect, 1 for defect)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Split your dataset into training and validation sets\n",
    "train_dataset, val_dataset = train_test_split(custom_dataset, train_size=0.8,  test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.0\n",
    "    lass_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        input, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    return lass_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.6934562321901322\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.6931471825242043\n",
      "LOSS train 0 valid 0.6931460499763489\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.6931471808552742\n",
      "LOSS train 0 valid 0.6931460499763489\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.6931471808552742\n",
      "LOSS train 0 valid 0.6931460499763489\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.6931471831202507\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 0.6931471830010414\n",
      "LOSS train 0 valid 0.6931460499763489\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 0.6931471830010414\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 0.6931471834778786\n",
      "LOSS train 0 valid 0.6931460499763489\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 0.6931471828222274\n",
      "LOSS train 0 valid 0.6931460499763489\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 0.6931471773982048\n",
      "LOSS train 0 valid 0.6931460499763489\n",
      "EPOCH 11:\n",
      "  batch 1000 loss: 0.69313340985775\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 12:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 13:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 14:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 15:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 16:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 17:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 18:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 19:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 20:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 21:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 22:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 23:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 24:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 25:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 26:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 27:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 28:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 29:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 30:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 31:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 32:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 33:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 34:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 35:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 36:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 37:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 38:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 39:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 40:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 41:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 42:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 43:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 44:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 45:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 46:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 47:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 48:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 49:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 50:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 51:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 52:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 53:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 54:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 55:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 56:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 57:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 58:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 59:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 60:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 61:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 62:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 63:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 64:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 65:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 66:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 67:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 68:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 69:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 70:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 71:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 72:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 73:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 74:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 75:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 76:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 77:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 78:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 79:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 80:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 81:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 82:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 83:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 84:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 85:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 86:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 87:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 88:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 89:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 90:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 91:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 92:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 93:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 94:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 95:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 96:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 97:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 98:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 99:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n",
      "EPOCH 100:\n",
      "  batch 1000 loss: 0.6931473016738892\n",
      "LOSS train 0 valid 0.6931461095809937\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/singnal_classification_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    # validation loss\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            val_input, vlabels = vdata\n",
    "            voutputs = model(val_input)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'models/model_{}_{}.pt'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"/home/tuandinh/Desktop/Signal Classification/file_data_new.csv\")\n",
    "# # data.shape\n",
    "# sample = data.iloc[1,:]\n",
    "# input = sample[:-1]\n",
    "# label = sample[-1]\n",
    "# input = torch.tensor(input)\n",
    "# input.shape\n",
    "\n",
    "# PATH = \"/home/tuandinh/Desktop/Signal Classification/models/model_20230910_093359_0.pt\"\n",
    "# model1 = SimpleClassifier(163, 32,2)\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint)\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# # Thực hiện dự đoán\n",
    "# with torch.no_grad():\n",
    "#     predicted = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/tuandinh/Desktop/Signal Classification/demo.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tuandinh/Desktop/Signal%20Classification/demo.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
