{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chuyển đổi thành công từ /home/tuandinh/Desktop/Signal Classification/data_signal_classification.txt thành /home/tuandinh/Desktop/Signal Classification/data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Đường dẫn đến tệp văn bản đầu vào\n",
    "input_txt_file = '/home/tuandinh/Desktop/Signal Classification/data_signal_classification.txt'\n",
    "\n",
    "# Đường dẫn đến tệp CSV đầu ra\n",
    "output_csv_file = '/home/tuandinh/Desktop/Signal Classification/data.csv'\n",
    "\n",
    "# Mở tệp văn bản đầu vào để đọc\n",
    "with open(input_txt_file, 'r') as txtfile:\n",
    "    # Đọc từng dòng trong tệp văn bản\n",
    "    lines = txtfile.readlines()\n",
    "\n",
    "# Mở tệp CSV đầu ra để viết\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    csvwriter.writerow(['Frequency', 'No_defect', 'with_defect'])\n",
    "\n",
    "    # Viết dữ liệu từ tệp văn bản vào tệp CSV\n",
    "    for line in lines:\n",
    "        # Tách dữ liệu bằng dấu phân tách của bạn (ví dụ: dấu tab '\\t')\n",
    "        data = line.strip().split('\\t')\n",
    "\n",
    "        # Viết dữ liệu vào tệp CSV\n",
    "        csvwriter.writerow(data)\n",
    "\n",
    "print(f\"Chuyển đổi thành công từ {input_txt_file} thành {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data_frame.iloc[index, :].values.astype(float)\n",
    "\n",
    "        # The last column can be the target label (0 for no defect, 1 for defect)\n",
    "        # You can adjust this based on your actual data structure\n",
    "        target = int(sample[-1])\n",
    "\n",
    "        # Remove the target label from the sample\n",
    "        sample = sample[:-1]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        # print(f\"Length of sample {len(sample)}\")\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        # Apply the optional data transformation\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "# Path to your merged CSV file\n",
    "# csv_merged = '/home/tuandinh/Desktop/Signal Classification/data_signal.csv'\n",
    "csv_merged = '/home/tuandinh/Desktop/Signal Classification/file_data_new.csv'\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = SignalDataset(csv_merged)\n",
    "# test_dataset = SignalDataset(csv_test_path)\n",
    "\n",
    "# # Create a data loader for the dataset\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a perceptron \n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(163, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)   \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleClassifier(\n",
       "  (fc1): Linear(in_features=163, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_size = 163  # Number of features in your data\n",
    "hidden_size = 64  # Number of hidden units in the neural network\n",
    "num_classes = 2  # Number of classes (0 for no defect, 1 for defect)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Split your dataset into training and validation sets\n",
    "train_dataset, val_dataset = train_test_split(custom_dataset, train_size=0.8,  test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.0\n",
    "    lass_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        input, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    return lass_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 2:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 3:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 4:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 5:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 6:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 7:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 8:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 9:\n",
      "LOSS train 0 valid 0.6931464076042175\n",
      "EPOCH 10:\n",
      "LOSS train 0 valid 0.6931464076042175\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/singnal_classification_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    # validation loss\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            val_input, vlabels = vdata\n",
    "            voutputs = model(val_input)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8197/3092540886.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sample = torch.tensor(data.iloc[1, :])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Thực hiện dự đoán\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 12\u001b[0m     predicted \u001b[39m=\u001b[39m model(sample)\n",
      "File \u001b[0;32m~/Desktop/Signal Classification/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[113], line 16\u001b[0m, in \u001b[0;36mSimpleClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)   \n",
      "File \u001b[0;32m~/Desktop/Signal Classification/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Signal Classification/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/tuandinh/Desktop/Signal Classification/test_only_defect.csv\")\n",
    "# data.shape\n",
    "sample = torch.tensor(data.iloc[1, :])\n",
    "\n",
    "PATH = \"/home/tuandinh/Desktop/Signal Classification/models/model_20230910_085122_0\"\n",
    "model1 = SimpleClassifier(163, 32,2)\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "# Thực hiện dự đoán\n",
    "with torch.no_grad():\n",
    "    predicted = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
